{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Installing Tensorflow Backbone first\n",
    "!pip install --upgrade tensorflow\n",
    "\n",
    "# Installing Keras Framework\n",
    "!pip install Keras\n",
    "\n",
    "# Installing necessary python packages\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (\"TensorFlow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, glob, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "TRAIN_BEN_IMAGE_PATH='./Train/0/'\n",
    "TRAIN_MAL_IMAGE_PATH='./Train/1/'\n",
    "\n",
    "VALID_BEN_IMAGE_PATH='./Valid/0/'\n",
    "VALID_MAL_IMAGE_PATH='./Valid/1/'\n",
    "\n",
    "TEST_BEN_IMAGE_PATH='./Test/0/'\n",
    "TEST_MAL_IMAGE_PATH='./Test/1/'\n",
    "\n",
    "train_ben_image_list = sorted(glob.glob(TRAIN_BEN_IMAGE_PATH + '*.png'))\n",
    "train_mal_image_list = sorted(glob.glob(TRAIN_MAL_IMAGE_PATH + '*.png'))\n",
    "\n",
    "valid_ben_image_list = sorted(glob.glob(VALID_BEN_IMAGE_PATH + '*.png'))\n",
    "valid_mal_image_list = sorted(glob.glob(VALID_MAL_IMAGE_PATH + '*.png'))\n",
    "\n",
    "test_ben_image_list = sorted(glob.glob(TEST_BEN_IMAGE_PATH + '*.png'))\n",
    "test_mal_image_list = sorted(glob.glob(TEST_MAL_IMAGE_PATH + '*.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying benign images\n",
    "ben_images = []\n",
    "for i in range(5):\n",
    "    ben_images.append(mpimg.imread(train_ben_image_list[i]))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "columns = 5\n",
    "for i, image in enumerate(ben_images):\n",
    "    plt.subplot(len(ben_images) / columns + 1, columns, i + 1)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying malignant images    \n",
    "mal_images = []\n",
    "for i in range(5):\n",
    "    mal_images.append(mpimg.imread(train_mal_image_list[i]))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "columns = 5\n",
    "for i, image in enumerate(mal_images):\n",
    "    plt.subplot(len(mal_images) / columns + 1, columns, i + 1)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants used for training\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 48\n",
    "CHANNELS = 3\n",
    "MODEL_WEIGHTS = './Model_Weights/'\n",
    "\n",
    "# Import necessary libraries\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Dropout, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, ReLU, Dense, Activation\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "# initialize the validation data augmentation object\n",
    "valAug = ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "# initialize the testing data augmentation object\n",
    "testAug = ImageDataGenerator(rescale=1 / 255.0)\n",
    "\n",
    "# initialize the training generator\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "    './Train/',\n",
    "    class_mode='binary',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    color_mode='rgb',\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "# initialize the validation generator\n",
    "valGen = valAug.flow_from_directory(\n",
    "    './Valid/',\n",
    "    class_mode='binary',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    color_mode='rgb',\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "# initialize the validation generator\n",
    "testGen = testAug.flow_from_directory(\n",
    "    './Test/',\n",
    "    class_mode='binary',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    color_mode='rgb',\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build our own model\n",
    "# Reference: https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/\n",
    "\n",
    "model = Sequential()\n",
    "inputShape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "\n",
    "# CONV => RELU => POOL layer set\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=inputShape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# (CONV => RELU) * 2 => POOL layer set\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# first (and only) set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# softmax classifier\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "print('Training the Model')\n",
    "optimizer = optimizers.Adam(learning_rate = LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mc = ModelCheckpoint(mode='max', filepath=MODEL_WEIGHTS + 'Weights_.{epoch:02d}-{val_accuracy:.4f}.h5', \n",
    "                     monitor='val_accuracy', save_best_only='True', save_weights_only='True', verbose=1)\n",
    "model.summary() \n",
    "\n",
    "total_num_train = len(train_ben_image_list) + len(train_mal_image_list)\n",
    "total_num_valid = len(valid_ben_image_list) + len(valid_mal_image_list)\n",
    "total_num_test = len(test_ben_image_list) + len(test_mal_image_list)\n",
    "\n",
    "steps_per_epoch = np.ceil(total_num_train / BATCH_SIZE).astype(\"int32\")\n",
    "print('Steps for Training Epoch: ', steps_per_epoch)\n",
    "\n",
    "validation_steps = np.ceil(total_num_valid / BATCH_SIZE).astype(\"int32\")\n",
    "print('Steps for Validation Epoch: ', validation_steps)\n",
    "\n",
    "testing_steps = np.ceil(total_num_test / BATCH_SIZE).astype(\"int32\")\n",
    "print('Steps for Testing: ', testing_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "history = model.fit_generator(trainGen,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              validation_data = valGen,\n",
    "                              validation_steps = validation_steps,\n",
    "                              epochs=NUM_EPOCHS,\n",
    "                              callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing training history\n",
    "N = np.arange(0, NUM_EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (Simple NN)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('./Trainig_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Testing network...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict_generator(testGen, steps=testing_steps)\n",
    "print(predIdxs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testGen.class_indices)\n",
    "predIdxs[predIdxs<0.5] = 0\n",
    "predIdxs[predIdxs>=0.5] = 1\n",
    "target_names = ['Benign', 'Malignant']\n",
    "print(classification_report(testGen.classes, predIdxs, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_weights_list = sorted(glob.glob(MODEL_WEIGHTS + '*.h5'), key=os.path.getmtime)\n",
    "print(saved_weights_list)\n",
    "print(saved_weights_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(saved_weights_list[-1])\n",
    "testGen.reset()\n",
    "predIdxs = model.predict_generator(testGen, steps=testing_steps)\n",
    "\n",
    "predIdxs[predIdxs<0.5] = 0\n",
    "predIdxs[predIdxs>=0.5] = 1\n",
    "\n",
    "target_names = ['Benign', 'Malignant']\n",
    "print(classification_report(testGen.classes, predIdxs, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -U efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.keras as efn \n",
    "eff = efn.EfficientNetB0(weights='imagenet', include_top=False, input_shape=inputShape)\n",
    "eff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = eff.input\n",
    "x = GlobalAveragePooling2D()(eff.output)\n",
    "print(x.get_shape())\n",
    "bsize, c = x.get_shape().as_list()\n",
    "x1 = Dense(c//2, activation='relu')(x)\n",
    "x2 = Dropout(0.5)(x1)\n",
    "x3 = Dense(1, activation='sigmoid')(x2)\n",
    "\n",
    "output = x3\n",
    "print('Output Shape: ', output.get_shape())\n",
    "\n",
    "model2 = Model(input, output)\n",
    "model2.name = 'EffNet_b1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()\n",
    "for layer in model2.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model2\n",
    "MODEL_WEIGHTS2 = './Model_Weights2/'\n",
    "print('Compiling the Model2')\n",
    "optimizer = optimizers.Adam(learning_rate = 0.01)\n",
    "model2.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mc = ModelCheckpoint(mode='max', filepath=MODEL_WEIGHTS2 + 'Frozen_Weights_.{epoch:02d}-{val_accuracy:.4f}.h5', \n",
    "                     monitor='val_accuracy', save_best_only='True', save_weights_only='True', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen.reset()\n",
    "valGen.reset()\n",
    "history = model2.fit_generator(trainGen,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              validation_data = valGen,\n",
    "                              validation_steps = validation_steps,\n",
    "                              epochs=1,\n",
    "                              callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in model2.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate = 0.01*0.1)\n",
    "model2.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mc = ModelCheckpoint(mode='max', filepath=MODEL_WEIGHTS2 + 'Unfrozen_Weights_.{epoch:02d}-{val_accuracy:.4f}.h5', \n",
    "                     monitor='val_accuracy', save_best_only='True', save_weights_only='True', verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen.reset()\n",
    "valGen.reset()\n",
    "history = model2.fit_generator(trainGen,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              validation_data = valGen,\n",
    "                              validation_steps = validation_steps,\n",
    "                              epochs=NUM_EPOCHS,\n",
    "                              callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_weights_list = sorted(glob.glob(MODEL_WEIGHTS2 + '*.h5'), key=os.path.getmtime)\n",
    "print(saved_weights_list)\n",
    "print(saved_weights_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights(saved_weights_list[-1])\n",
    "testGen.reset()\n",
    "predIdxs = model2.predict_generator(testGen, steps=testing_steps)\n",
    "\n",
    "predIdxs[predIdxs<0.5] = 0\n",
    "predIdxs[predIdxs>=0.5] = 1\n",
    "\n",
    "target_names = ['Benign', 'Malignant']\n",
    "print(classification_report(testGen.classes, predIdxs, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_workshop]",
   "language": "python",
   "name": "conda-env-keras_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
